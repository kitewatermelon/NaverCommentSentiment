import asyncio
import pandas as pd
import re
from pathlib import Path
from playwright.async_api import async_playwright
from tqdm.asyncio import tqdm_asyncio

# -------------------------
# URLì—ì„œ news_id ì¶”ì¶œ
# -------------------------
def extract_news_id(url):
    match = re.search(r"/article/comment/(\d+)/(\d+)", url)
    if match:
        return f"{match.group(1)}-{match.group(2)}"
    return "unknown"

# -------------------------
# URLë³„ ëŒ“ê¸€ + ì‘ì„±ì¼ ìˆ˜ì§‘
# -------------------------
async def scrape_comments_from_url(context, url):
    page = await context.new_page()
    comments = []

    try:
        await page.goto(url, timeout=20000)
        await page.wait_for_selector("ul.u_cbox_list, span.u_cbox_contents_none", timeout=10000)

        # ëŒ“ê¸€ ì—†ëŠ” ê²½ìš°
        no_comment = await page.query_selector("span.u_cbox_contents_none")
        if no_comment:
            text = (await no_comment.inner_text()).strip()
            if "ë“±ë¡ëœ ëŒ“ê¸€ì´ ì—†ìŠµë‹ˆë‹¤." in text:
                await page.close()
                return extract_news_id(url), []

        # "ë”ë³´ê¸°" ë°˜ë³µ í´ë¦­
        while True:
            paginate_div = await page.query_selector("div.u_cbox_paginate")
            if not paginate_div:
                break
            style = await paginate_div.get_attribute("style") or ""
            if "display: none" in style:
                break
            more_btn = await paginate_div.query_selector("span.u_cbox_page_more")
            if not more_btn:
                break
            await more_btn.click()
            await page.wait_for_timeout(500)

        # ëŒ“ê¸€ + ì‘ì„±ì¼ ìˆ˜ì§‘
        comment_elements = await page.query_selector_all("ul.u_cbox_list li")  # li ë‹¨ìœ„ë¡œ ë°˜ë³µ
        for el in comment_elements:
            try:
                text_el = await el.query_selector(".u_cbox_contents")
                date_el = await el.query_selector(".u_cbox_date")

                text = (await text_el.inner_text()).strip() if text_el else ""
                date = (await date_el.inner_text()).strip() if date_el else ""

                if text:
                    comments.append({"comment": text, "datetime": date})
            except:
                continue

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ ({url}): {e}")

    await page.close()
    return extract_news_id(url), comments

# -------------------------
# ì—‘ì…€ ê¸°ë°˜ ë³‘ë ¬ ì²˜ë¦¬ + CSV ì¦‰ì‹œ ì €ì¥
# -------------------------
async def scrape_comments_from_excel_parallel_to_csv(excel_path, output_csv="comments.csv", max_concurrency=5):
    df = pd.read_excel(excel_path)
    if "link" not in df.columns:
        print("âŒ ì—‘ì…€ì— 'link' ì—´ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    urls = df["link"].dropna().tolist()
    print(f"ì´ {len(urls)}ê°œ URLì—ì„œ ëŒ“ê¸€ ìˆ˜ì§‘ ì‹œì‘")

    # CSV íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ ì²´í¬
    output_path = Path(output_csv)
    first_write = not output_path.exists()

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context()

        semaphore = asyncio.Semaphore(max_concurrency)

        async def sem_task(url):
            async with semaphore:
                return await scrape_comments_from_url(context, url)

        tasks = [sem_task(url) for url in urls]

        for coro in tqdm_asyncio.as_completed(tasks):
            news_id, comments = await coro
            if comments:
                df_save = pd.DataFrame([{
                    "news_id": news_id,
                    "comment": c["comment"],
                    "datetime": c["datetime"]
                } for c in comments])
                df_save.to_csv(output_path, mode='a', index=False,
                               header=first_write, encoding="utf-8-sig")
                first_write = False  # ì´í›„ë¶€í„°ëŠ” í—¤ë” ì—†ì´ append
            print(f"âœ… {news_id} ëŒ“ê¸€ ìˆ˜ì§‘ ì™„ë£Œ: {len(comments)}ê°œ")

        await browser.close()

    print(f"ğŸ‰ ëŒ“ê¸€ ì €ì¥ ì™„ë£Œ â†’ {output_csv}")

# -------------------------
# ì—¬ëŸ¬ ì—‘ì…€ íŒŒì¼ ë™ì‹œ ì‹¤í–‰
# -------------------------
async def main():
    tasks = [
        scrape_comments_from_excel_parallel_to_csv(
            r"output\ìœ¤ì„ì—´íƒ„í•µ_ë„¤ì´ë²„ë‰´ìŠ¤.xlsx",
            output_csv="output/ìœ¤ì„ì—´íƒ„í•µ_ë„¤ì´ë²„ë‰´ìŠ¤_ëŒ“ê¸€.csv",
            max_concurrency=5
        ),
        scrape_comments_from_excel_parallel_to_csv(
            r"output\ë°•ê·¼í˜œíƒ„í•µ_ë„¤ì´ë²„ë‰´ìŠ¤.xlsx",
            output_csv="output/ë°•ê·¼í˜œíƒ„í•µ_ë„¤ì´ë²„ë‰´ìŠ¤_ëŒ“ê¸€.csv",
            max_concurrency=5
        )
    ]
    await asyncio.gather(*tasks)

if __name__ == "__main__":
    asyncio.run(main())
